"""
FastAPI server for Chatterbox Multilingual TTS
Converted from Gradio app for RunPod deployment
"""

import os
import io
import random
import numpy as np
import torch
from pathlib import Path
from typing import Optional, List, Literal
import logging

from fastapi import FastAPI, File, UploadFile, HTTPException, Form
from fastapi.responses import StreamingResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import soundfile as sf

from src.chatterbox.mtl_tts import ChatterboxMultilingualTTS, SUPPORTED_LANGUAGES

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Device configuration
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"ðŸš€ Running on device: {DEVICE}")

# Global model
MODEL = None

# Reference audio storage
REFERENCE_AUDIO_DIR = Path("./reference_audio")
REFERENCE_AUDIO_DIR.mkdir(exist_ok=True)

# Language config with default audio URLs
LANGUAGE_CONFIG = {
    "ar": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ar_f/ar_prompts2.flac",
        "text": "ÙÙŠ Ø§Ù„Ø´Ù‡Ø± Ø§Ù„Ù…Ø§Ø¶ÙŠØŒ ÙˆØµÙ„Ù†Ø§ Ø¥Ù„Ù‰ Ù…Ø¹Ù„Ù… Ø¬Ø¯ÙŠØ¯ Ø¨Ù…Ù„ÙŠØ§Ø±ÙŠÙ† Ù…Ù† Ø§Ù„Ù…Ø´Ø§Ù‡Ø¯Ø§Øª Ø¹Ù„Ù‰ Ù‚Ù†Ø§ØªÙ†Ø§ Ø¹Ù„Ù‰ ÙŠÙˆØªÙŠÙˆØ¨."
    },
    "da": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/da_m1.flac",
        "text": "Sidste mÃ¥ned nÃ¥ede vi en ny milepÃ¦l med to milliarder visninger pÃ¥ vores YouTube-kanal."
    },
    "de": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/de_f1.flac",
        "text": "Letzten Monat haben wir einen neuen Meilenstein erreicht: zwei Milliarden Aufrufe auf unserem YouTube-Kanal."
    },
    "el": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/el_m.flac",
        "text": "Î¤Î¿Î½ Ï€ÎµÏÎ±ÏƒÎ¼Î­Î½Î¿ Î¼Î®Î½Î±, Ï†Ï„Î¬ÏƒÎ±Î¼Îµ ÏƒÎµ Î­Î½Î± Î½Î­Î¿ Î¿ÏÏŒÏƒÎ·Î¼Î¿ Î¼Îµ Î´ÏÎ¿ Î´Î¹ÏƒÎµÎºÎ±Ï„Î¿Î¼Î¼ÏÏÎ¹Î± Ï€ÏÎ¿Î²Î¿Î»Î­Ï‚ ÏƒÏ„Î¿ ÎºÎ±Î½Î¬Î»Î¹ Î¼Î±Ï‚ ÏƒÏ„Î¿ YouTube."
    },
    "en": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/en_f1.flac",
        "text": "Last month, we reached a new milestone with two billion views on our YouTube channel."
    },
    "es": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/es_f1.flac",
        "text": "El mes pasado alcanzamos un nuevo hito: dos mil millones de visualizaciones en nuestro canal de YouTube."
    },
    "fi": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/fi_m.flac",
        "text": "Viime kuussa saavutimme uuden virstanpylvÃ¤Ã¤n kahden miljardin katselukerran kanssa YouTube-kanavallamme."
    },
    "fr": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/fr_f1.flac",
        "text": "Le mois dernier, nous avons atteint un nouveau jalon avec deux milliards de vues sur notre chaÃ®ne YouTube."
    },
    "he": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/he_m1.flac",
        "text": "×‘×—×•×“×© ×©×¢×‘×¨ ×”×’×¢× ×• ×œ××‘×Ÿ ×“×¨×š ×—×“×©×” ×¢× ×©× ×™ ×ž×™×œ×™××¨×“ ×¦×¤×™×•×ª ×‘×¢×¨×•×¥ ×”×™×•×˜×™×•×‘ ×©×œ× ×•."
    },
    "hi": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/hi_f1.flac",
        "text": "à¤ªà¤¿à¤›à¤²à¥‡ à¤®à¤¹à¥€à¤¨à¥‡ à¤¹à¤®à¤¨à¥‡ à¤à¤• à¤¨à¤¯à¤¾ à¤®à¥€à¤² à¤•à¤¾ à¤ªà¤¤à¥à¤¥à¤° à¤›à¥à¤†: à¤¹à¤®à¤¾à¤°à¥‡ YouTube à¤šà¥ˆà¤¨à¤² à¤ªà¤° à¤¦à¥‹ à¤…à¤°à¤¬ à¤µà¥à¤¯à¥‚à¤œà¤¼à¥¤"
    },
    "it": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/it_m1.flac",
        "text": "Il mese scorso abbiamo raggiunto un nuovo traguardo: due miliardi di visualizzazioni sul nostro canale YouTube."
    },
    "ja": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ja/ja_prompts1.flac",
        "text": "å…ˆæœˆã€ç§ãŸã¡ã®YouTubeãƒãƒ£ãƒ³ãƒãƒ«ã§äºŒåå„„å›žã®å†ç”Ÿå›žæ•°ã¨ã„ã†æ–°ãŸãªãƒžã‚¤ãƒ«ã‚¹ãƒˆãƒ¼ãƒ³ã«åˆ°é”ã—ã¾ã—ãŸã€‚"
    },
    "ko": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ko_f.flac",
        "text": "ì§€ë‚œë‹¬ ìš°ë¦¬ëŠ” ìœ íŠœë¸Œ ì±„ë„ì—ì„œ ì´ì‹­ì–µ ì¡°íšŒìˆ˜ë¼ëŠ” ìƒˆë¡œìš´ ì´ì •í‘œì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤."
    },
    "ms": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ms_f.flac",
        "text": "Bulan lepas, kami mencapai pencapaian baru dengan dua bilion tontonan di saluran YouTube kami."
    },
    "nl": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/nl_m.flac",
        "text": "Vorige maand bereikten we een nieuwe mijlpaal met twee miljard weergaven op ons YouTube-kanaal."
    },
    "no": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/no_f1.flac",
        "text": "Forrige mÃ¥ned nÃ¥dde vi en ny milepÃ¦l med to milliarder visninger pÃ¥ YouTube-kanalen vÃ¥r."
    },
    "pl": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/pl_m.flac",
        "text": "W zeszÅ‚ym miesiÄ…cu osiÄ…gnÄ™liÅ›my nowy kamieÅ„ milowy z dwoma miliardami wyÅ›wietleÅ„ na naszym kanale YouTube."
    },
    "pt": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/pt_m1.flac",
        "text": "No mÃªs passado, alcanÃ§Ã¡mos um novo marco: dois mil milhÃµes de visualizaÃ§Ãµes no nosso canal do YouTube."
    },
    "ru": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/ru_m.flac",
        "text": "Ð’ Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð¼ Ð¼ÐµÑÑÑ†Ðµ Ð¼Ñ‹ Ð´Ð¾ÑÑ‚Ð¸Ð³Ð»Ð¸ Ð½Ð¾Ð²Ð¾Ð³Ð¾ Ñ€ÑƒÐ±ÐµÐ¶Ð°: Ð´Ð²Ð° Ð¼Ð¸Ð»Ð»Ð¸Ð°Ñ€Ð´Ð° Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€Ð¾Ð² Ð½Ð° Ð½Ð°ÑˆÐµÐ¼ YouTube-ÐºÐ°Ð½Ð°Ð»Ðµ."
    },
    "sv": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/sv_f.flac",
        "text": "FÃ¶rra mÃ¥naden nÃ¥dde vi en ny milstolpe med tvÃ¥ miljarder visningar pÃ¥ vÃ¥r YouTube-kanal."
    },
    "sw": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/sw_m.flac",
        "text": "Mwezi uliopita, tulifika hatua mpya ya maoni ya bilioni mbili kweny kituo chetu cha YouTube."
    },
    "tr": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/tr_m.flac",
        "text": "GeÃ§en ay YouTube kanalÄ±mÄ±zda iki milyar gÃ¶rÃ¼ntÃ¼leme ile yeni bir dÃ¶nÃ¼m noktasÄ±na ulaÅŸtÄ±k."
    },
    "zh": {
        "audio": "https://storage.googleapis.com/chatterbox-demo-samples/mtl_prompts/zh_f2.flac",
        "text": "ä¸Šä¸ªæœˆï¼Œæˆ‘ä»¬è¾¾åˆ°äº†ä¸€ä¸ªæ–°çš„é‡Œç¨‹ç¢‘ã€‚ æˆ‘ä»¬çš„YouTubeé¢‘é“è§‚çœ‹æ¬¡æ•°è¾¾åˆ°äº†äºŒåäº¿æ¬¡ï¼Œè¿™ç»å¯¹ä»¤äººéš¾ä»¥ç½®ä¿¡ã€‚"
    },
}

# Initialize FastAPI app
app = FastAPI(
    title="Chatterbox Multilingual TTS API",
    description="Generate high-quality multilingual speech from text with reference audio styling",
    version="1.0.0"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response models

# New simplified request model (HF Space style)
class TTSRequest(BaseModel):
    text: str = Field(..., description="Text to synthesize (max 300 characters)", max_length=300)
    language_id: str = Field(..., description="Language code (e.g., 'en', 'tr', 'ar')")
    audio_prompt_path: Optional[str] = Field(None, description="Path to reference audio file (optional)")
    exaggeration: float = Field(0.5, ge=0.25, le=2.0, description="Speech expressiveness (0.25-2.0)")
    temperature: float = Field(0.8, ge=0.05, le=5.0, description="Randomness in generation (0.05-5.0)")
    seed: int = Field(0, description="Random seed (0 for random)")
    cfg_weight: float = Field(0.5, ge=0.2, le=1.0, description="CFG/Pace weight (0.2-1.0)")

# Legacy request model (Chatterbox-TTS-Server compatibility)
class CustomTTSRequest(BaseModel):
    """Request model for the custom /tts endpoint (backward compatibility)."""
    text: str = Field(..., min_length=1, description="Text to be synthesized.")
    voice_mode: Literal["predefined", "clone"] = Field("predefined", description="Voice mode")
    predefined_voice_id: Optional[str] = Field(None, description="Filename of predefined voice")
    reference_audio_filename: Optional[str] = Field(None, description="Filename of reference audio for cloning")
    output_format: Optional[Literal["wav", "opus", "mp3"]] = Field("wav", description="Audio output format")
    split_text: Optional[bool] = Field(True, description="Whether to split long text into chunks")
    chunk_size: Optional[int] = Field(120, ge=50, le=500, description="Target chunk size")
    temperature: Optional[float] = Field(None, description="Temperature override")
    exaggeration: Optional[float] = Field(None, description="Exaggeration override")
    cfg_weight: Optional[float] = Field(None, description="CFG weight override")
    seed: Optional[int] = Field(None, description="Seed override")
    speed_factor: Optional[float] = Field(None, description="Speed factor override")
    language: Optional[str] = Field(None, description="Language code override")

class LanguageResponse(BaseModel):
    languages: dict
    count: int

class HealthResponse(BaseModel):
    status: str
    device: str
    model_loaded: bool

# Helper functions
def get_or_load_model():
    """Loads the ChatterboxMultilingualTTS model if it hasn't been loaded already,
    and ensures it's on the correct device."""
    global MODEL
    if MODEL is None:
        logger.info("Model not loaded, initializing...")
        try:
            # HF Space uses DEVICE as string, but from_pretrained expects torch.device
            # However, it can accept string too, so we'll use string like Gradio
            MODEL = ChatterboxMultilingualTTS.from_pretrained(DEVICE)
            # Ensure model is on correct device (same as Gradio)
            if hasattr(MODEL, 'to') and str(MODEL.device) != DEVICE:
                MODEL.to(DEVICE)
            logger.info(f"Model loaded successfully. Internal device: {getattr(MODEL, 'device', 'N/A')}")
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
    return MODEL

def set_seed(seed: int):
    """Sets the random seed for reproducibility."""
    torch.manual_seed(seed)
    if DEVICE == "cuda":
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    random.seed(seed)
    np.random.seed(seed)

def resolve_audio_prompt(language_id: str, provided_path: Optional[str]) -> Optional[str]:
    """Decide which audio prompt to use."""
    if provided_path and str(provided_path).strip():
        # Check if it's a local file path
        local_path = REFERENCE_AUDIO_DIR / provided_path
        if local_path.exists():
            return str(local_path)
        # Otherwise assume it's a URL or absolute path
        return provided_path
    # Fall back to language-specific default
    return LANGUAGE_CONFIG.get(language_id, {}).get("audio")

# Load model at startup
@app.on_event("startup")
async def startup_event():
    try:
        get_or_load_model()
        logger.info("âœ… Model loaded successfully at startup")
    except Exception as e:
        logger.error(f"âŒ Failed to load model on startup: {e}")

# API Endpoints
@app.get("/", tags=["Info"])
async def root():
    """Root endpoint with API information."""
    return {
        "name": "Chatterbox Multilingual TTS API",
        "version": "1.0.0",
        "description": "Generate high-quality multilingual speech from text",
        "supported_languages": len(SUPPORTED_LANGUAGES),
        "endpoints": {
            "POST /generate": "Generate speech from text",
            "POST /upload_reference": "Upload reference audio file",
            "GET /languages": "Get supported languages",
            "GET /health": "Health check",
            "GET /references": "List uploaded reference audio files"
        }
    }

@app.get("/health", response_model=HealthResponse, tags=["Health"])
async def health_check():
    """Health check endpoint."""
    return HealthResponse(
        status="healthy" if MODEL is not None else "model_not_loaded",
        device=DEVICE,
        model_loaded=MODEL is not None
    )

@app.get("/languages", response_model=LanguageResponse, tags=["Languages"])
async def get_languages():
    """Get list of supported languages."""
    return LanguageResponse(
        languages=SUPPORTED_LANGUAGES,
        count=len(SUPPORTED_LANGUAGES)
    )

@app.get("/references", tags=["References"])
async def list_references():
    """List all uploaded reference audio files."""
    files = []
    for file_path in REFERENCE_AUDIO_DIR.glob("*"):
        if file_path.is_file():
            files.append({
                "filename": file_path.name,
                "size": file_path.stat().st_size,
                "path": str(file_path.relative_to(Path(".")))
            })
    return {"files": files, "count": len(files)}

@app.post("/upload_reference", tags=["References"])
async def upload_reference(
    file: UploadFile = File(..., description="Audio file to upload"),
    name: Optional[str] = Form(None, description="Custom filename (optional)")
):
    """
    Upload a reference audio file for voice cloning.
    Supported formats: WAV, MP3, FLAC, etc.
    """
    try:
        # Validate file type
        if not file.content_type or not file.content_type.startswith("audio/"):
            raise HTTPException(status_code=400, detail="File must be an audio file")

        # Determine filename
        filename = name or file.filename
        if not filename:
            raise HTTPException(status_code=400, detail="Filename is required")

        # Save file
        file_path = REFERENCE_AUDIO_DIR / filename
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)

        logger.info(f"Uploaded reference audio: {filename} ({len(content)} bytes)")

        return {
            "status": "success",
            "filename": filename,
            "path": str(file_path.relative_to(Path("."))),
            "size": len(content)
        }
    except Exception as e:
        logger.error(f"Error uploading reference audio: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to upload file: {str(e)}")

@app.post("/generate", tags=["TTS"])
async def generate_tts(request: TTSRequest):
    """
    Generate speech audio from text.

    This endpoint synthesizes natural-sounding speech from input text.
    When a reference audio file is provided, it captures the speaker's voice characteristics.
    """
    try:
        # Validate language
        if request.language_id.lower() not in SUPPORTED_LANGUAGES:
            raise HTTPException(
                status_code=400,
                detail=f"Unsupported language_id '{request.language_id}'. Supported: {', '.join(SUPPORTED_LANGUAGES.keys())}"
            )

        # Get model
        model = get_or_load_model()
        if model is None:
            raise HTTPException(status_code=503, detail="TTS model is not loaded")

        # Set seed if provided
        if request.seed != 0:
            set_seed(request.seed)

        logger.info(f"Generating audio for text: '{request.text[:50]}...' (lang: {request.language_id})")

        # Resolve audio prompt
        audio_prompt = resolve_audio_prompt(request.language_id, request.audio_prompt_path)

        # Prepare generation kwargs
        generate_kwargs = {
            "exaggeration": request.exaggeration,
            "temperature": request.temperature,
            "cfg_weight": request.cfg_weight,
        }
        if audio_prompt:
            generate_kwargs["audio_prompt_path"] = audio_prompt
            logger.info(f"Using audio prompt: {audio_prompt}")
        else:
            logger.info("No audio prompt provided; using default voice")

        # Generate audio (same as Gradio - language_id passed directly, model handles lowercase internally)
        wav = model.generate(
            request.text[:300],  # Truncate text to max chars
            language_id=request.language_id,  # Model handles lowercase internally
            **generate_kwargs
        )

        # Convert to numpy array
        audio_np = wav.squeeze(0).numpy()
        sample_rate = model.sr

        logger.info(f"Audio generation complete. Sample rate: {sample_rate}Hz, Length: {len(audio_np)} samples")

        # Create audio buffer
        audio_buffer = io.BytesIO()
        sf.write(audio_buffer, audio_np, sample_rate, format="WAV")
        audio_buffer.seek(0)

        return StreamingResponse(
            audio_buffer,
            media_type="audio/wav",
            headers={
                "Content-Disposition": f'attachment; filename="tts_output.wav"',
                "X-Sample-Rate": str(sample_rate)
            }
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error generating TTS: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to generate audio: {str(e)}")

@app.post("/tts", tags=["TTS"], summary="Legacy /tts endpoint (backward compatibility)")
async def legacy_tts_endpoint(request: CustomTTSRequest):
    """
    Legacy /tts endpoint for backward compatibility with Chatterbox-TTS-Server.
    Converts CustomTTSRequest to TTSRequest format and calls generate_tts.
    """
    try:
        # Resolve audio prompt path based on voice_mode
        audio_prompt_path = None
        if request.voice_mode == "predefined" and request.predefined_voice_id:
            # Check if predefined voice exists in reference_audio directory
            predefined_path = REFERENCE_AUDIO_DIR / request.predefined_voice_id
            if predefined_path.exists():
                audio_prompt_path = str(predefined_path)
            else:
                # Try to use language default
                language_to_use = request.language or "en"
                audio_prompt_path = LANGUAGE_CONFIG.get(language_to_use, {}).get("audio")
        elif request.voice_mode == "clone" and request.reference_audio_filename:
            clone_path = REFERENCE_AUDIO_DIR / request.reference_audio_filename
            if clone_path.exists():
                audio_prompt_path = str(clone_path)
            else:
                raise HTTPException(
                    status_code=404,
                    detail=f"Reference audio file '{request.reference_audio_filename}' not found"
                )
        else:
            # Use language default
            language_to_use = request.language or "en"
            audio_prompt_path = LANGUAGE_CONFIG.get(language_to_use, {}).get("audio")

        # Handle text chunking if enabled
        text_to_synthesize = request.text
        if request.split_text and len(request.text) > request.chunk_size:
            # Simple chunking by sentences (basic implementation)
            sentences = text_to_synthesize.split('. ')
            chunks = []
            current_chunk = ""
            for sentence in sentences:
                if len(current_chunk) + len(sentence) < request.chunk_size:
                    current_chunk += sentence + ". "
                else:
                    if current_chunk:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + ". "
            if current_chunk:
                chunks.append(current_chunk.strip())
            text_to_synthesize = chunks[0] if chunks else text_to_synthesize  # Use first chunk for simplicity
        
        # Convert to TTSRequest format
        tts_request = TTSRequest(
            text=text_to_synthesize[:300],  # Max 300 chars
            language_id=request.language or "en",
            audio_prompt_path=audio_prompt_path,
            exaggeration=request.exaggeration or 0.5,
            temperature=request.temperature or 0.8,
            seed=request.seed or 0,
            cfg_weight=request.cfg_weight or 0.5
        )
        
        # Call generate endpoint
        return await generate_tts(tts_request)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in legacy /tts endpoint: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to generate audio: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8004))  # Default port 8004 (same as Chatterbox-TTS-Server)
    uvicorn.run(app, host="0.0.0.0", port=port)

